{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5200b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7686a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/minseok_EDA2_train.csv')\n",
    "test = pd.read_csv('../data/minseok_EDA2_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47102a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>hand</th>\n",
       "      <th>religion</th>\n",
       "      <th>orientation</th>\n",
       "      <th>voted</th>\n",
       "      <th>married</th>\n",
       "      <th>familysize</th>\n",
       "      <th>ASD</th>\n",
       "      <th>nerdiness</th>\n",
       "      <th>Qs_Mach</th>\n",
       "      <th>age_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.219430</td>\n",
       "      <td>0.684909</td>\n",
       "      <td>0.645424</td>\n",
       "      <td>0.689224</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.580272</td>\n",
       "      <td>0.649525</td>\n",
       "      <td>0.664648</td>\n",
       "      <td>0.287443</td>\n",
       "      <td>0.462615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550968</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.617225</td>\n",
       "      <td>0.549084</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>0.547405</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>1</td>\n",
       "      <td>3.653846</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.549147</td>\n",
       "      <td>0.490676</td>\n",
       "      <td>0.505430</td>\n",
       "      <td>0.576165</td>\n",
       "      <td>0.538382</td>\n",
       "      <td>0.754382</td>\n",
       "      <td>0.494185</td>\n",
       "      <td>0.537294</td>\n",
       "      <td>0.460687</td>\n",
       "      <td>0.462615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.597924</td>\n",
       "      <td>0.521066</td>\n",
       "      <td>0.562683</td>\n",
       "      <td>0.515796</td>\n",
       "      <td>0.547405</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>1</td>\n",
       "      <td>3.269231</td>\n",
       "      <td>0.523114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.549147</td>\n",
       "      <td>0.684909</td>\n",
       "      <td>0.645424</td>\n",
       "      <td>0.576165</td>\n",
       "      <td>0.490014</td>\n",
       "      <td>0.754382</td>\n",
       "      <td>0.649525</td>\n",
       "      <td>0.664648</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.597924</td>\n",
       "      <td>0.600253</td>\n",
       "      <td>0.549084</td>\n",
       "      <td>0.529865</td>\n",
       "      <td>0.547405</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>1</td>\n",
       "      <td>3.692308</td>\n",
       "      <td>0.523114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.549147</td>\n",
       "      <td>0.490676</td>\n",
       "      <td>0.505430</td>\n",
       "      <td>0.382335</td>\n",
       "      <td>0.538382</td>\n",
       "      <td>0.427424</td>\n",
       "      <td>0.435168</td>\n",
       "      <td>0.664648</td>\n",
       "      <td>0.460687</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550968</td>\n",
       "      <td>0.565894</td>\n",
       "      <td>0.521066</td>\n",
       "      <td>0.549084</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>1</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.564478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.549147</td>\n",
       "      <td>0.490676</td>\n",
       "      <td>0.505430</td>\n",
       "      <td>0.576165</td>\n",
       "      <td>0.490014</td>\n",
       "      <td>0.427424</td>\n",
       "      <td>0.494185</td>\n",
       "      <td>0.413060</td>\n",
       "      <td>0.460687</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550968</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.521066</td>\n",
       "      <td>0.549084</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>0.573536</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>0</td>\n",
       "      <td>3.461538</td>\n",
       "      <td>0.564478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.302370</td>\n",
       "      <td>0.684909</td>\n",
       "      <td>0.505430</td>\n",
       "      <td>0.440718</td>\n",
       "      <td>0.490014</td>\n",
       "      <td>0.580272</td>\n",
       "      <td>0.494185</td>\n",
       "      <td>0.537294</td>\n",
       "      <td>0.460687</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.565894</td>\n",
       "      <td>0.578521</td>\n",
       "      <td>0.549084</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>0.544640</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>0</td>\n",
       "      <td>3.346154</td>\n",
       "      <td>0.564478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.696582</td>\n",
       "      <td>0.490676</td>\n",
       "      <td>0.645424</td>\n",
       "      <td>0.576165</td>\n",
       "      <td>0.538382</td>\n",
       "      <td>0.754382</td>\n",
       "      <td>0.649525</td>\n",
       "      <td>0.537294</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.641272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.495370</td>\n",
       "      <td>0.521066</td>\n",
       "      <td>0.562683</td>\n",
       "      <td>0.515796</td>\n",
       "      <td>0.544640</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>1</td>\n",
       "      <td>4.038462</td>\n",
       "      <td>0.523114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.549147</td>\n",
       "      <td>0.684909</td>\n",
       "      <td>0.645424</td>\n",
       "      <td>0.689224</td>\n",
       "      <td>0.679836</td>\n",
       "      <td>0.754382</td>\n",
       "      <td>0.649525</td>\n",
       "      <td>0.664648</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.641272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.565894</td>\n",
       "      <td>0.600253</td>\n",
       "      <td>0.562683</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>0.544640</td>\n",
       "      <td>0.664114</td>\n",
       "      <td>1</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.696582</td>\n",
       "      <td>0.684909</td>\n",
       "      <td>0.505430</td>\n",
       "      <td>0.689224</td>\n",
       "      <td>0.679836</td>\n",
       "      <td>0.754382</td>\n",
       "      <td>0.649525</td>\n",
       "      <td>0.313116</td>\n",
       "      <td>0.675138</td>\n",
       "      <td>0.641272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.617225</td>\n",
       "      <td>0.549084</td>\n",
       "      <td>0.515796</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.664114</td>\n",
       "      <td>0</td>\n",
       "      <td>4.115385</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>0.696582</td>\n",
       "      <td>0.490676</td>\n",
       "      <td>0.344569</td>\n",
       "      <td>0.689224</td>\n",
       "      <td>0.384088</td>\n",
       "      <td>0.347197</td>\n",
       "      <td>0.494185</td>\n",
       "      <td>0.413060</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.597924</td>\n",
       "      <td>0.600253</td>\n",
       "      <td>0.549084</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>0.573536</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>1</td>\n",
       "      <td>3.423077</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Q1        Q2        Q3        Q4        Q5        Q6        Q7  \\\n",
       "0      0.219430  0.684909  0.645424  0.689224  0.270300  0.580272  0.649525   \n",
       "1      0.549147  0.490676  0.505430  0.576165  0.538382  0.754382  0.494185   \n",
       "2      0.549147  0.684909  0.645424  0.576165  0.490014  0.754382  0.649525   \n",
       "3      0.549147  0.490676  0.505430  0.382335  0.538382  0.427424  0.435168   \n",
       "4      0.549147  0.490676  0.505430  0.576165  0.490014  0.427424  0.494185   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14995  0.302370  0.684909  0.505430  0.440718  0.490014  0.580272  0.494185   \n",
       "14996  0.696582  0.490676  0.645424  0.576165  0.538382  0.754382  0.649525   \n",
       "14997  0.549147  0.684909  0.645424  0.689224  0.679836  0.754382  0.649525   \n",
       "14998  0.696582  0.684909  0.505430  0.689224  0.679836  0.754382  0.649525   \n",
       "14999  0.696582  0.490676  0.344569  0.689224  0.384088  0.347197  0.494185   \n",
       "\n",
       "             Q8        Q9       Q10  ...      hand  religion  orientation  \\\n",
       "0      0.664648  0.287443  0.462615  ...  0.550968  0.567721     0.617225   \n",
       "1      0.537294  0.460687  0.462615  ...  0.552290  0.597924     0.521066   \n",
       "2      0.664648  0.541905  0.523798  ...  0.552290  0.597924     0.600253   \n",
       "3      0.664648  0.460687  0.523798  ...  0.550968  0.565894     0.521066   \n",
       "4      0.413060  0.460687  0.523798  ...  0.550968  0.567721     0.521066   \n",
       "...         ...       ...       ...  ...       ...       ...          ...   \n",
       "14995  0.537294  0.460687  0.523798  ...  0.552290  0.565894     0.578521   \n",
       "14996  0.537294  0.541905  0.641272  ...  0.552290  0.495370     0.521066   \n",
       "14997  0.664648  0.541905  0.641272  ...  0.552290  0.565894     0.600253   \n",
       "14998  0.313116  0.675138  0.641272  ...  0.552290  0.567721     0.617225   \n",
       "14999  0.413060  0.541905  0.523798  ...  0.552290  0.597924     0.600253   \n",
       "\n",
       "          voted   married  familysize       ASD  nerdiness   Qs_Mach   age_cat  \n",
       "0      0.549084  0.560104    0.547405  0.546760          1  3.653846  0.559000  \n",
       "1      0.562683  0.515796    0.547405  0.546760          1  3.269231  0.523114  \n",
       "2      0.549084  0.529865    0.547405  0.546760          1  3.692308  0.523114  \n",
       "3      0.549084  0.560104    0.565604  0.546760          1  3.500000  0.564478  \n",
       "4      0.549084  0.560104    0.573536  0.546760          0  3.461538  0.564478  \n",
       "...         ...       ...         ...       ...        ...       ...       ...  \n",
       "14995  0.549084  0.560104    0.544640  0.546760          0  3.346154  0.564478  \n",
       "14996  0.562683  0.515796    0.544640  0.546760          1  4.038462  0.523114  \n",
       "14997  0.562683  0.560104    0.544640  0.664114          1  4.615385  0.559000  \n",
       "14998  0.549084  0.515796    0.565604  0.664114          0  4.115385  0.559000  \n",
       "14999  0.549084  0.560104    0.573536  0.546760          1  3.423077  0.559000  \n",
       "\n",
       "[15000 rows x 69 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094dbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in train.columns.to_list() if col not in ['nerdiness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a73c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train[columns]\n",
    "target = train['nerdiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c7420b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, data=data, target=target):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2, random_state=20171184)\n",
    "    param = {\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree' : trial.suggest_categorical('colsample_bytree', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-2, 0.2),\n",
    "        'n_estimators' : trial.suggest_categorical('colsample_bytree', [1000, 3000, 5000, 7000, 10000]),\n",
    "        'max_depth' : trial.suggest_categorical('max_depth', [3, 5, 6, 7, 8, 9, 10, 12]),\n",
    "        'random_state' : trial.suggest_categorical('random_state', [42, 777, 20171184]),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'use_label_encoder' : False,\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "    preds = model.predict(test_x)\n",
    "    \n",
    "    auc = roc_auc_score(test_y, preds)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2da92bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-07 20:27:23,481]\u001b[0m A new study created in memory with name: no-name-3ef0cc18-90c9-423a-beca-f3ff11c42c4f\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:27:25,787]\u001b[0m Trial 0 finished with value: 0.7059696533995599 and parameters: {'lambda': 0.006799621068806167, 'alpha': 0.06308880026837554, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.1295283977001047, 'n_estimators': 7119, 'max_depth': 8, 'random_state': 777, 'min_child_weight': 195}. Best is trial 0 with value: 0.7059696533995599.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:27:38,225]\u001b[0m Trial 1 finished with value: 0.7684138291614926 and parameters: {'lambda': 0.9369654913096985, 'alpha': 0.061509452376463165, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.036876683150603315, 'n_estimators': 5723, 'max_depth': 12, 'random_state': 42, 'min_child_weight': 17}. Best is trial 1 with value: 0.7684138291614926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:27:40,166]\u001b[0m Trial 2 finished with value: 0.7000468378973053 and parameters: {'lambda': 0.2633728334939361, 'alpha': 0.0012027895132367523, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.07140540668024974, 'n_estimators': 6670, 'max_depth': 6, 'random_state': 42, 'min_child_weight': 270}. Best is trial 1 with value: 0.7684138291614926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:27:42,215]\u001b[0m Trial 3 finished with value: 0.714234363767074 and parameters: {'lambda': 0.05551084763430547, 'alpha': 0.17190859595187252, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.10465602836138961, 'n_estimators': 7492, 'max_depth': 12, 'random_state': 777, 'min_child_weight': 95}. Best is trial 1 with value: 0.7684138291614926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:06,086]\u001b[0m Trial 4 finished with value: 0.7486683876870793 and parameters: {'lambda': 0.005981725084940117, 'alpha': 1.585108844345066, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.01209902636707751, 'n_estimators': 5674, 'max_depth': 6, 'random_state': 20171184, 'min_child_weight': 56}. Best is trial 1 with value: 0.7684138291614926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:07,276]\u001b[0m Trial 5 finished with value: 0.7083061019976908 and parameters: {'lambda': 0.47957999811714486, 'alpha': 0.007078828810313297, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.15017771682368386, 'n_estimators': 6278, 'max_depth': 10, 'random_state': 42, 'min_child_weight': 252}. Best is trial 1 with value: 0.7684138291614926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:09,242]\u001b[0m Trial 6 finished with value: 0.7017025031043723 and parameters: {'lambda': 1.3011849531535689, 'alpha': 0.013466581713664296, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.04545299553998413, 'n_estimators': 5276, 'max_depth': 8, 'random_state': 20171184, 'min_child_weight': 173}. Best is trial 1 with value: 0.7684138291614926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:20,280]\u001b[0m Trial 7 finished with value: 0.7724086661002548 and parameters: {'lambda': 0.003741189672443248, 'alpha': 1.022789580397131, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.0557903704656932, 'n_estimators': 9091, 'max_depth': 9, 'random_state': 777, 'min_child_weight': 8}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:21,945]\u001b[0m Trial 8 finished with value: 0.70189312245387 and parameters: {'lambda': 0.005407893738776898, 'alpha': 0.28991462824344666, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.03900549930576355, 'n_estimators': 7575, 'max_depth': 8, 'random_state': 777, 'min_child_weight': 196}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:23,665]\u001b[0m Trial 9 finished with value: 0.698486482364987 and parameters: {'lambda': 0.0031925345203731117, 'alpha': 0.014634763875927182, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.07790779844529143, 'n_estimators': 8797, 'max_depth': 8, 'random_state': 20171184, 'min_child_weight': 270}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:32,494]\u001b[0m Trial 10 finished with value: 0.720745376119208 and parameters: {'lambda': 0.0010399732779827477, 'alpha': 4.225877763624607, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.017508437796332654, 'n_estimators': 2240, 'max_depth': 9, 'random_state': 777, 'min_child_weight': 100}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:28:41,366]\u001b[0m Trial 11 finished with value: 0.7277574450471647 and parameters: {'lambda': 9.903408094694962, 'alpha': 0.9010460877560237, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.030578599253068696, 'n_estimators': 2994, 'max_depth': 3, 'random_state': 42, 'min_child_weight': 7}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:29:05,487]\u001b[0m Trial 12 finished with value: 0.7636537916911749 and parameters: {'lambda': 0.04824931809694428, 'alpha': 8.899006165218468, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.02527588585884196, 'n_estimators': 9951, 'max_depth': 7, 'random_state': 42, 'min_child_weight': 6}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:29:08,429]\u001b[0m Trial 13 finished with value: 0.7322315534932358 and parameters: {'lambda': 1.2934476901829894, 'alpha': 0.0718349381980088, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.05662441310814373, 'n_estimators': 502, 'max_depth': 12, 'random_state': 42, 'min_child_weight': 58}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:29:25,392]\u001b[0m Trial 14 finished with value: 0.7561651743894735 and parameters: {'lambda': 9.307971284944633, 'alpha': 0.4291865817250151, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.023176038719082994, 'n_estimators': 3441, 'max_depth': 9, 'random_state': 777, 'min_child_weight': 42}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:29:27,847]\u001b[0m Trial 15 finished with value: 0.7282476090887304 and parameters: {'lambda': 0.16395186228044661, 'alpha': 1.6085712486816952, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.19745146276244782, 'n_estimators': 9675, 'max_depth': 9, 'random_state': 777, 'min_child_weight': 111}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:29:36,882]\u001b[0m Trial 16 finished with value: 0.7240675990675991 and parameters: {'lambda': 0.019926359446524444, 'alpha': 0.04121831313136272, 'colsample_bytree': 1.0, 'subsample': 1.0, 'learning_rate': 0.04249574895065976, 'n_estimators': 4231, 'max_depth': 5, 'random_state': 42, 'min_child_weight': 140}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:29:46,301]\u001b[0m Trial 17 finished with value: 0.761412652767793 and parameters: {'lambda': 2.3702290537308, 'alpha': 0.5499105355169702, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.06666111934214858, 'n_estimators': 9003, 'max_depth': 12, 'random_state': 42, 'min_child_weight': 35}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:29:56,921]\u001b[0m Trial 18 finished with value: 0.7157892730322637 and parameters: {'lambda': 0.01934673830100829, 'alpha': 0.0021967966457333977, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.015437271707422677, 'n_estimators': 8498, 'max_depth': 3, 'random_state': 777, 'min_child_weight': 79}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:30:16,467]\u001b[0m Trial 19 finished with value: 0.7698761518854976 and parameters: {'lambda': 0.4369817254292677, 'alpha': 0.14633790171631267, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.032679347954641065, 'n_estimators': 4626, 'max_depth': 10, 'random_state': 20171184, 'min_child_weight': 3}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n",
      "\u001b[32m[I 2022-08-07 20:30:20,735]\u001b[0m Trial 20 finished with value: 0.7292115983704768 and parameters: {'lambda': 0.1160286681810312, 'alpha': 2.4342734405096325, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.09940896826576685, 'n_estimators': 4259, 'max_depth': 10, 'random_state': 20171184, 'min_child_weight': 123}. Best is trial 7 with value: 0.7724086661002548.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24416/3864377705.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maximize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of finished trials:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best trial:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24416/1053172295.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial, data, target)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1174\u001b[0m         )\n\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m   1177\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1499\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1500\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32019dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_alpha</th>\n",
       "      <th>params_colsample_bytree</th>\n",
       "      <th>params_lambda</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_max_depth</th>\n",
       "      <th>params_min_child_weight</th>\n",
       "      <th>params_n_estimators</th>\n",
       "      <th>params_random_state</th>\n",
       "      <th>params_subsample</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.705970</td>\n",
       "      <td>2022-08-07 20:27:23.482525</td>\n",
       "      <td>2022-08-07 20:27:25.786042</td>\n",
       "      <td>0 days 00:00:02.303517</td>\n",
       "      <td>0.063089</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.129528</td>\n",
       "      <td>8</td>\n",
       "      <td>195</td>\n",
       "      <td>7119</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.768414</td>\n",
       "      <td>2022-08-07 20:27:25.787042</td>\n",
       "      <td>2022-08-07 20:27:38.225225</td>\n",
       "      <td>0 days 00:00:12.438183</td>\n",
       "      <td>0.061509</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.936965</td>\n",
       "      <td>0.036877</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>5723</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.700047</td>\n",
       "      <td>2022-08-07 20:27:38.226225</td>\n",
       "      <td>2022-08-07 20:27:40.166525</td>\n",
       "      <td>0 days 00:00:01.940300</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.263373</td>\n",
       "      <td>0.071405</td>\n",
       "      <td>6</td>\n",
       "      <td>270</td>\n",
       "      <td>6670</td>\n",
       "      <td>42</td>\n",
       "      <td>0.6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.714234</td>\n",
       "      <td>2022-08-07 20:27:40.167525</td>\n",
       "      <td>2022-08-07 20:27:42.215902</td>\n",
       "      <td>0 days 00:00:02.048377</td>\n",
       "      <td>0.171909</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.055511</td>\n",
       "      <td>0.104656</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>7492</td>\n",
       "      <td>777</td>\n",
       "      <td>0.4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.748668</td>\n",
       "      <td>2022-08-07 20:27:42.216902</td>\n",
       "      <td>2022-08-07 20:28:06.086336</td>\n",
       "      <td>0 days 00:00:23.869434</td>\n",
       "      <td>1.585109</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>5674</td>\n",
       "      <td>20171184</td>\n",
       "      <td>0.9</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.708306</td>\n",
       "      <td>2022-08-07 20:28:06.087336</td>\n",
       "      <td>2022-08-07 20:28:07.275901</td>\n",
       "      <td>0 days 00:00:01.188565</td>\n",
       "      <td>0.007079</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.479580</td>\n",
       "      <td>0.150178</td>\n",
       "      <td>10</td>\n",
       "      <td>252</td>\n",
       "      <td>6278</td>\n",
       "      <td>42</td>\n",
       "      <td>0.6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.701703</td>\n",
       "      <td>2022-08-07 20:28:07.276901</td>\n",
       "      <td>2022-08-07 20:28:09.242081</td>\n",
       "      <td>0 days 00:00:01.965180</td>\n",
       "      <td>0.013467</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.301185</td>\n",
       "      <td>0.045453</td>\n",
       "      <td>8</td>\n",
       "      <td>173</td>\n",
       "      <td>5276</td>\n",
       "      <td>20171184</td>\n",
       "      <td>0.4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.772409</td>\n",
       "      <td>2022-08-07 20:28:09.243081</td>\n",
       "      <td>2022-08-07 20:28:20.280772</td>\n",
       "      <td>0 days 00:00:11.037691</td>\n",
       "      <td>1.022790</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.055790</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9091</td>\n",
       "      <td>777</td>\n",
       "      <td>0.7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.701893</td>\n",
       "      <td>2022-08-07 20:28:20.281772</td>\n",
       "      <td>2022-08-07 20:28:21.945184</td>\n",
       "      <td>0 days 00:00:01.663412</td>\n",
       "      <td>0.289915</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.039005</td>\n",
       "      <td>8</td>\n",
       "      <td>196</td>\n",
       "      <td>7575</td>\n",
       "      <td>777</td>\n",
       "      <td>0.5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.698486</td>\n",
       "      <td>2022-08-07 20:28:21.946184</td>\n",
       "      <td>2022-08-07 20:28:23.665575</td>\n",
       "      <td>0 days 00:00:01.719391</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.077908</td>\n",
       "      <td>8</td>\n",
       "      <td>270</td>\n",
       "      <td>8797</td>\n",
       "      <td>20171184</td>\n",
       "      <td>0.5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.720745</td>\n",
       "      <td>2022-08-07 20:28:23.666575</td>\n",
       "      <td>2022-08-07 20:28:32.494606</td>\n",
       "      <td>0 days 00:00:08.828031</td>\n",
       "      <td>4.225878</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.017508</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>2240</td>\n",
       "      <td>777</td>\n",
       "      <td>0.7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.727757</td>\n",
       "      <td>2022-08-07 20:28:32.495606</td>\n",
       "      <td>2022-08-07 20:28:41.365458</td>\n",
       "      <td>0 days 00:00:08.869852</td>\n",
       "      <td>0.901046</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.903408</td>\n",
       "      <td>0.030579</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2994</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.763654</td>\n",
       "      <td>2022-08-07 20:28:41.366458</td>\n",
       "      <td>2022-08-07 20:29:05.487324</td>\n",
       "      <td>0 days 00:00:24.120866</td>\n",
       "      <td>8.899006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9951</td>\n",
       "      <td>42</td>\n",
       "      <td>0.7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.732232</td>\n",
       "      <td>2022-08-07 20:29:05.488324</td>\n",
       "      <td>2022-08-07 20:29:08.429986</td>\n",
       "      <td>0 days 00:00:02.941662</td>\n",
       "      <td>0.071835</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.293448</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.756165</td>\n",
       "      <td>2022-08-07 20:29:08.430986</td>\n",
       "      <td>2022-08-07 20:29:25.392530</td>\n",
       "      <td>0 days 00:00:16.961544</td>\n",
       "      <td>0.429187</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.307971</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>3441</td>\n",
       "      <td>777</td>\n",
       "      <td>0.7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.728248</td>\n",
       "      <td>2022-08-07 20:29:25.393531</td>\n",
       "      <td>2022-08-07 20:29:27.847678</td>\n",
       "      <td>0 days 00:00:02.454147</td>\n",
       "      <td>1.608571</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.163952</td>\n",
       "      <td>0.197451</td>\n",
       "      <td>9</td>\n",
       "      <td>111</td>\n",
       "      <td>9675</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.724068</td>\n",
       "      <td>2022-08-07 20:29:27.848678</td>\n",
       "      <td>2022-08-07 20:29:36.881721</td>\n",
       "      <td>0 days 00:00:09.033043</td>\n",
       "      <td>0.041218</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019926</td>\n",
       "      <td>0.042496</td>\n",
       "      <td>5</td>\n",
       "      <td>140</td>\n",
       "      <td>4231</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.761413</td>\n",
       "      <td>2022-08-07 20:29:36.882721</td>\n",
       "      <td>2022-08-07 20:29:46.301262</td>\n",
       "      <td>0 days 00:00:09.418541</td>\n",
       "      <td>0.549911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.370229</td>\n",
       "      <td>0.066661</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>9003</td>\n",
       "      <td>42</td>\n",
       "      <td>0.9</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>2022-08-07 20:29:46.302262</td>\n",
       "      <td>2022-08-07 20:29:56.921499</td>\n",
       "      <td>0 days 00:00:10.619237</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>8498</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.769876</td>\n",
       "      <td>2022-08-07 20:29:56.922500</td>\n",
       "      <td>2022-08-07 20:30:16.466949</td>\n",
       "      <td>0 days 00:00:19.544449</td>\n",
       "      <td>0.146338</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.436982</td>\n",
       "      <td>0.032679</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4626</td>\n",
       "      <td>20171184</td>\n",
       "      <td>0.7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.729212</td>\n",
       "      <td>2022-08-07 20:30:16.467950</td>\n",
       "      <td>2022-08-07 20:30:20.734242</td>\n",
       "      <td>0 days 00:00:04.266292</td>\n",
       "      <td>2.434273</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.116029</td>\n",
       "      <td>0.099409</td>\n",
       "      <td>10</td>\n",
       "      <td>123</td>\n",
       "      <td>4259</td>\n",
       "      <td>20171184</td>\n",
       "      <td>0.7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-07 20:30:20.735243</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.154450</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.499131</td>\n",
       "      <td>0.033029</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1438</td>\n",
       "      <td>20171184</td>\n",
       "      <td>0.7</td>\n",
       "      <td>RUNNING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "0        0  0.705970 2022-08-07 20:27:23.482525 2022-08-07 20:27:25.786042   \n",
       "1        1  0.768414 2022-08-07 20:27:25.787042 2022-08-07 20:27:38.225225   \n",
       "2        2  0.700047 2022-08-07 20:27:38.226225 2022-08-07 20:27:40.166525   \n",
       "3        3  0.714234 2022-08-07 20:27:40.167525 2022-08-07 20:27:42.215902   \n",
       "4        4  0.748668 2022-08-07 20:27:42.216902 2022-08-07 20:28:06.086336   \n",
       "5        5  0.708306 2022-08-07 20:28:06.087336 2022-08-07 20:28:07.275901   \n",
       "6        6  0.701703 2022-08-07 20:28:07.276901 2022-08-07 20:28:09.242081   \n",
       "7        7  0.772409 2022-08-07 20:28:09.243081 2022-08-07 20:28:20.280772   \n",
       "8        8  0.701893 2022-08-07 20:28:20.281772 2022-08-07 20:28:21.945184   \n",
       "9        9  0.698486 2022-08-07 20:28:21.946184 2022-08-07 20:28:23.665575   \n",
       "10      10  0.720745 2022-08-07 20:28:23.666575 2022-08-07 20:28:32.494606   \n",
       "11      11  0.727757 2022-08-07 20:28:32.495606 2022-08-07 20:28:41.365458   \n",
       "12      12  0.763654 2022-08-07 20:28:41.366458 2022-08-07 20:29:05.487324   \n",
       "13      13  0.732232 2022-08-07 20:29:05.488324 2022-08-07 20:29:08.429986   \n",
       "14      14  0.756165 2022-08-07 20:29:08.430986 2022-08-07 20:29:25.392530   \n",
       "15      15  0.728248 2022-08-07 20:29:25.393531 2022-08-07 20:29:27.847678   \n",
       "16      16  0.724068 2022-08-07 20:29:27.848678 2022-08-07 20:29:36.881721   \n",
       "17      17  0.761413 2022-08-07 20:29:36.882721 2022-08-07 20:29:46.301262   \n",
       "18      18  0.715789 2022-08-07 20:29:46.302262 2022-08-07 20:29:56.921499   \n",
       "19      19  0.769876 2022-08-07 20:29:56.922500 2022-08-07 20:30:16.466949   \n",
       "20      20  0.729212 2022-08-07 20:30:16.467950 2022-08-07 20:30:20.734242   \n",
       "21      21       NaN 2022-08-07 20:30:20.735243                        NaT   \n",
       "\n",
       "                 duration  params_alpha  params_colsample_bytree  \\\n",
       "0  0 days 00:00:02.303517      0.063089                      0.7   \n",
       "1  0 days 00:00:12.438183      0.061509                      0.7   \n",
       "2  0 days 00:00:01.940300      0.001203                      0.7   \n",
       "3  0 days 00:00:02.048377      0.171909                      0.7   \n",
       "4  0 days 00:00:23.869434      1.585109                      0.5   \n",
       "5  0 days 00:00:01.188565      0.007079                      0.9   \n",
       "6  0 days 00:00:01.965180      0.013467                      0.4   \n",
       "7  0 days 00:00:11.037691      1.022790                      0.8   \n",
       "8  0 days 00:00:01.663412      0.289915                      0.7   \n",
       "9  0 days 00:00:01.719391      0.014635                      0.5   \n",
       "10 0 days 00:00:08.828031      4.225878                      0.8   \n",
       "11 0 days 00:00:08.869852      0.901046                      0.8   \n",
       "12 0 days 00:00:24.120866      8.899006                      1.0   \n",
       "13 0 days 00:00:02.941662      0.071835                      0.6   \n",
       "14 0 days 00:00:16.961544      0.429187                      0.8   \n",
       "15 0 days 00:00:02.454147      1.608571                      0.4   \n",
       "16 0 days 00:00:09.033043      0.041218                      1.0   \n",
       "17 0 days 00:00:09.418541      0.549911                      0.9   \n",
       "18 0 days 00:00:10.619237      0.002197                      0.6   \n",
       "19 0 days 00:00:19.544449      0.146338                      0.8   \n",
       "20 0 days 00:00:04.266292      2.434273                      0.8   \n",
       "21                    NaT      0.154450                      0.8   \n",
       "\n",
       "    params_lambda  params_learning_rate  params_max_depth  \\\n",
       "0        0.006800              0.129528                 8   \n",
       "1        0.936965              0.036877                12   \n",
       "2        0.263373              0.071405                 6   \n",
       "3        0.055511              0.104656                12   \n",
       "4        0.005982              0.012099                 6   \n",
       "5        0.479580              0.150178                10   \n",
       "6        1.301185              0.045453                 8   \n",
       "7        0.003741              0.055790                 9   \n",
       "8        0.005408              0.039005                 8   \n",
       "9        0.003193              0.077908                 8   \n",
       "10       0.001040              0.017508                 9   \n",
       "11       9.903408              0.030579                 3   \n",
       "12       0.048249              0.025276                 7   \n",
       "13       1.293448              0.056624                12   \n",
       "14       9.307971              0.023176                 9   \n",
       "15       0.163952              0.197451                 9   \n",
       "16       0.019926              0.042496                 5   \n",
       "17       2.370229              0.066661                12   \n",
       "18       0.019347              0.015437                 3   \n",
       "19       0.436982              0.032679                10   \n",
       "20       0.116029              0.099409                10   \n",
       "21       0.499131              0.033029                10   \n",
       "\n",
       "    params_min_child_weight  params_n_estimators  params_random_state  \\\n",
       "0                       195                 7119                  777   \n",
       "1                        17                 5723                   42   \n",
       "2                       270                 6670                   42   \n",
       "3                        95                 7492                  777   \n",
       "4                        56                 5674             20171184   \n",
       "5                       252                 6278                   42   \n",
       "6                       173                 5276             20171184   \n",
       "7                         8                 9091                  777   \n",
       "8                       196                 7575                  777   \n",
       "9                       270                 8797             20171184   \n",
       "10                      100                 2240                  777   \n",
       "11                        7                 2994                   42   \n",
       "12                        6                 9951                   42   \n",
       "13                       58                  502                   42   \n",
       "14                       42                 3441                  777   \n",
       "15                      111                 9675                  777   \n",
       "16                      140                 4231                   42   \n",
       "17                       35                 9003                   42   \n",
       "18                       79                 8498                  777   \n",
       "19                        3                 4626             20171184   \n",
       "20                      123                 4259             20171184   \n",
       "21                        5                 1438             20171184   \n",
       "\n",
       "    params_subsample     state  \n",
       "0                0.8  COMPLETE  \n",
       "1                0.8  COMPLETE  \n",
       "2                0.6  COMPLETE  \n",
       "3                0.4  COMPLETE  \n",
       "4                0.9  COMPLETE  \n",
       "5                0.6  COMPLETE  \n",
       "6                0.4  COMPLETE  \n",
       "7                0.7  COMPLETE  \n",
       "8                0.5  COMPLETE  \n",
       "9                0.5  COMPLETE  \n",
       "10               0.7  COMPLETE  \n",
       "11               0.8  COMPLETE  \n",
       "12               0.7  COMPLETE  \n",
       "13               1.0  COMPLETE  \n",
       "14               0.7  COMPLETE  \n",
       "15               0.8  COMPLETE  \n",
       "16               1.0  COMPLETE  \n",
       "17               0.9  COMPLETE  \n",
       "18               0.8  COMPLETE  \n",
       "19               0.7  COMPLETE  \n",
       "20               0.7  COMPLETE  \n",
       "21               0.7   RUNNING  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fca0eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda': 0.003741189672443248,\n",
       " 'alpha': 1.022789580397131,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'subsample': 0.7,\n",
       " 'learning_rate': 0.0557903704656932,\n",
       " 'n_estimators': 9091,\n",
       " 'max_depth': 9,\n",
       " 'random_state': 777,\n",
       " 'min_child_weight': 8,\n",
       " 'tree_method': 'gpu_hist'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Best_trial = study.best_trial.params\n",
    "Best_trial[\"tree_method\"] = 'gpu_hist'\n",
    "Best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b797e67e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:31:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fold: 1 ==> auc: 0.7710090495539977\n",
      "[20:31:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 2 ==> auc: 0.7895254603574805\n",
      "[20:31:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 3 ==> auc: 0.7877452236604655\n",
      "[20:31:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 4 ==> auc: 0.7799394261151256\n",
      "[20:31:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 5 ==> auc: 0.7755677983337262\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros(test.shape[0])\n",
    "kf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "auc=[]\n",
    "n=0\n",
    "for trn_idx, test_idx in kf.split(train[columns], train['nerdiness']):\n",
    "    X_tr, X_val = train[columns].iloc[trn_idx], train[columns].iloc[test_idx]\n",
    "    y_tr, y_val = train['nerdiness'].iloc[trn_idx], train['nerdiness'].iloc[test_idx]\n",
    "    model = xgb.XGBClassifier(**Best_trial)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "    preds += model.predict(test[columns])/kf.n_splits\n",
    "    auc.append(roc_auc_score(y_val, model.predict(X_val)))\n",
    "    print(f'fold: {n+1} ==> auc: {auc[n]}')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f3b7378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7807573916041591"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c38127",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473dc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d15f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['nerdiness']=preds\n",
    "sub.to_csv('../submission/xgboost_optuna.csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
